# Task 1 引言篇
## 1.1 什么是语言模型
> 感觉不少地方拗口和难以理解

> 头一次接触语言模型这个东西，感觉资料中的描述有些复杂，我这里先写下自己简单的感性理解：

- 语言模型（Language Model）：给一定数量的词汇排序，每种排序方式LM都会给出一个概率，这个概率可以理解为这些词汇拼接成的句子在人类日常生活中出现的可能性，越接近人类日常生活用语，概率越大。
- 也就是说语⾔模型可以接受⼀个序列并返回⼀个概率来评估其“好坏”。  
> 语言模型也可以进行生成任务，最简单的是从语言模型中进行采样。

- 自回归语言模型（Autoregressive language models）是一种用来描述序列联合分布的常见方法，通过使用概率的链式法则来表示。在自回归语言模型中，我们需要理解每个记号在给定前面记号的条件下，下一个记号的概率分布。在数学上，任何联合概率分布都可以使用这种方式表示。但是，自回归语言模型的特点是它可以利用前馈神经网络等方法有效地计算每个条件概率分布。
> 链式分布：

![chrome_EcG6SiClRx.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694415059072-44336635-71bb-4cb7-9022-eea3f6f439f5.png#averageHue=%23f9f9f9&clientId=ufb87a45b-e227-4&from=paste&height=96&id=uc2ce48f0&originHeight=108&originWidth=811&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=10624&status=done&style=none&taskId=u01505e2c-5498-4ee1-ac19-77b95e927a6&title=&width=720.8888888888889)
> 下面是关于非自回归生成任务的内容，看得出有模型退火算法，文字叙述难以理解，暂且搁置。

##  1.2 大模型相关历史回顾  
> 这里介绍了熵的概念，很难以理解，我从其他地方学习了熵的概念

- 熵的计算公式：H(X) = - Σ P(x) * log2(P(x))，其中，H(X)表示随机变量X的熵，P(x)表示X取值为x的概率。
- 以骰子为例，每面出现的概率如下：P(1) = 1/6；P(2) = 1/6；P(3) = 1/6；P(4) = 1/6；P(5) = 1/6；P(6) = 1/6
- 带入公式得到：H(X) = - log2(1/6)，大约为2.58496比特（bits）
- 这个结果表示了骰子的信息熵，由于骰子是均匀的，所以熵到达了最大值，如果骰子不均匀，某个面的概率更高，那么熵就会减少，不确定性降低了。
- 硬币均匀的信息熵为：H(X) = - (0.5) * log2(0.5) - (0.5) * log2(0.5)，计算结果约为1比特（bits）；假设不均匀，其中一面概率是0.8，则：H(X) = - (0.8) * log2(0.8) - (0.2) * log2(0.2)，计算结果约为0.721928比特（bits）。可见，在硬币不均匀的情况下，不确定性更小，只猜概率大的那面猜中的可能性更大。
> 香农特别对测量英语的熵感兴趣，将其表示为一系列的字母。这意味着我们想象存在一个“真实”的分布p（这种存在是有问题的，但它仍然是一个有用的数学抽象），它能产生英语文本样本x ~ p。
> 交叉熵：

![chrome_HQXrgelLOU.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694417999938-6d6080f5-c485-424d-82c4-21cc79de78ba.png#averageHue=%23f5f5f5&clientId=ufb87a45b-e227-4&from=paste&height=89&id=u64c190e9&originHeight=100&originWidth=405&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=6812&status=done&style=none&taskId=uffbc474f-2ac0-4c7d-92f3-b2185947fe1&title=&width=360)
> N-gram是一种基于统计语言模型的算法，用于预测下一个词的概率。在N-gram模型中，对于一个给定的词序列x_{1:i−1}（其中x_i是当前位置的词），模型将根据其前面的n−1个词来预测第i个词的概率。具体来说，对于任意一个x_i，模型都会考虑所有可能的前n−1个词的组合，然后计算每个组合的概率，最后选择概率最大的那个作为x_i的预测概率。

![chrome_qK4VBTRtfo.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694418089412-3ca89a42-b2a2-4027-ac93-4ea8a963c308.png#averageHue=%23fcfbf9&clientId=ufb87a45b-e227-4&from=paste&height=50&id=u7d49772b&originHeight=56&originWidth=366&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=3736&status=done&style=none&taskId=u34c49c41-2620-46fc-bb17-593291dab3c&title=&width=325.3333333333333)
> 例如，对于一个Trigram模型（N=3），我们想要计算下一个词的概率，需要考虑前两个词的上下文。假设我们有一个句子："I love to"，我们想要计算下一个词 "code" 出现的概率。根据Trigram模型，我们可以计算条件概率：P("code" | "I love to")为了计算这个条件概率，我们需要从语料库中统计出现过的 "I love to code" 这个Trigram序列的频次，并将其除以 "I love to" 出现的次数，即可以得到该条件概率。


>  语⾔模型的⼀个重要进步是神经⽹络的引⼊。Bengio等⼈在2003年 ⾸次提出了神经语⾔模型，其中 由神经⽹络给 出：  

![chrome_noTXe6x34k.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694418334668-b30a2dff-47bd-4ee2-856d-9f6486993bc6.png#averageHue=%23f5f5f5&clientId=ufb87a45b-e227-4&from=paste&height=68&id=uc4940006&originHeight=77&originWidth=797&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=11107&status=done&style=none&taskId=uc7a1d165-8fb1-43c5-9701-3caec420ec1&title=&width=708.4444444444445)
 ⾃2003年以来，神经语⾔建模的两个关键发展包括：Recurrent Neural Networks（RNNs）和Transformers。
# Task 2 大模型的能力
> GPT-3是一个强大的语言模型，虽然没有特别针对任务训练，但在广泛的NLP任务中表现不错。对于特定任务，合理利用大量标签数据可以超越当前技术水平。

## 2.1  语言模型的适应性
> 在自然语言处理中，语言模型是一种用于对token序列进行建模的技术。它可以对序列进行评估，如生成概率最高的下一个词。此外，语言模型还可以在给定提示的条件下生成完整的序列。

> 在任务驱动的自然语言处理中，任务是将输入映射到输出。例如，在问答的情境下，将问题映射到答案。

> 对于适应语言模型，可采取训练和提示两种方法。训练阶段的适应可以使用有监督学习，创建新模型或更新现有模型。提示阶段可以利用上下文信息，通过构建一组提示输入到语言模型中来生成完成的序列。

结果表明，更多的训练实例和更大的模型总是更好的选择。我们选择的任务包括：

- 语言模型
- 问答
- 翻译
- 算术
- 新闻文章生成
- 新颖任务
1. 在自然语言处理（NLP）中，除了对大型语言模型的考虑，还需要回顾一些基本任务，如对GPT-3的功能和提示工程的了解。
2. 语言模型是关于词汇序列的概率分布，困惑度是衡量语言模型性能的重要指标，表示模型在预测下一个词时的平均不确定性。
3. 困惑度可以解释为每个标记（token）的平均"分支因子"，衡量模型预测的多样性和不确定性。
4. 算术平均在困惑度计算中不会惩罚给定词标记概率为0的情况，而几何平均（困惑度）会降低整体分数。
5. 困惑度可以理解为平均编码长度，表示在给定当前词或标记后，下一个词或标记可能的选择数量。
6. 语言模型可能犯召回错误和精确度错误两种类型的错误，困惑度对这两种错误的处理方式不对称。
7. HellaSwag是一个常识推理评估数据集，GPT-3在该数据集上接近但没有超过最先进的水平。
8. 根据实验结果，GPT-3在多个任务和数据集上的表现相对优秀，即使在没有针对特定数据集进行训练的情况下也取得了良好的结果。
## 2.2 问答任务

- 闭卷问答任务，输入是一个问题，输出是一个答案。
- 语言模型需要“知道”答案，而无需在数据库或文档中查找信息。

**TriviaQA**

- 任务：给定问题生成答案。
- 原始数据集用于开放式阅读理解挑战，但在闭卷问答任务中使用。
- 根据训练实例和问题定义一个提示，将生成的内容作为预测答案。

**WebQuestions**

- 任务：类似于TriviaQA的问答任务。
- 数据集从Google搜索查询中收集，最初用于知识库的问题回答。
- 根据问题定义一个提示。

**NaturalQuestions**

- 任务：回答问题。
- 数据集从Google搜索查询中收集，答案较长。
- 根据问题定义一个提示。

阅读笔记：
## 2.3 翻译

1. 翻译任务是将源语言中的句子翻译成目标语言中的句子。
2. 机器翻译自20世纪60年代以来就是一个具有丰富数据集的长期任务。
3. 统计机器翻译在2000年代开始迅速发展，紧随其后是2010年代中期的神经机器翻译。
4. 常用的评估数据集包括WMT'14和WMT'16数据集，自动评估指标是BLEU，捕捉了n-gram重叠的概念。
5. Few-shot情况下，构造了输入-输出训练实例和输入提示来评估模型。
6. GPT-3在德语到英语翻译任务中，即使没有监督训练数据，也能达到全监督系统的最新技术水平。

## 2.4 算术

1. 在抽象推理任务中评估GPT-3性能之一是算术题（2-5位数的加法、减法、乘法）。
2. 这个任务只是用来满足科学好奇心的诊断任务，并没有实际应用的理由。
3. 从实验结果看，虽然GPT-3的结果并不理想，但仍然令人惊叹，并充满了对未来的想象。
## 2.5 新闻文章生成

1. 任务：给定标题和副标题，生成新闻文章。
2. 数据集来自newser.com的标题和副标题。
3. 设立了评估标准，人类根据文章可能由机器编写的可能性对其进行评分。
4. 在上下文学习中给模型提供提示样本。
5. 人类只有52%的时间能够正确地区分“人类”和“机器”。
## 2.6 新颖任务
**使用新词**

1. 任务：给定一个新造的词和定义，生成使用该词的句子。
2. 通过提示来描述任务。

**纠正英语语法**
标题：United Methodists Agree to Historic Split
副标题：Those who oppose gay marriage will form their own denomination
文章：经过两天的激烈辩论，联合卫理公会（United Methodist Church）同意历史性的分裂，预计将导致新教派的建立，该教派在神学和社会上均保守，根据《华盛顿邮报》的说法。5月份参加教会年度全议会的多数代表投票加强禁止LGBTQ牧师的掌权，并制定新的规定，将“惩罚”主持同性婚礼的牧师。但反对这些措施的人有一个新计划：他们表示将在2020年组建一个单独的派别，称他们的教会为基督卫理公会（Christian Methodist denomination）…
“screeg”是指用剑砍击某物。一个使用screeg一词的句子示例是：我们用剑砍击了树。
任务：给定一个不合语法的句子，生成其合语法的版本。
通过给出提示来描述任务（提示包括输入和输入对）。
## 2.7 总结

- GPT-3在⼴泛的标准NLP基准测试和⼀次性任务上进⾏了评估。
- GPT-3可以表现得极好或者⾮常普通。
- 增加模型的⼤⼩和示例的数量都有助于提⾼性能。
- 有⼀些启发式的⽅法可以将语⾔模型适应到感兴趣的任务。
- 但是为什么会有这样表现，没有⼈知道。
# Task 3 大模型的有害性
本章中，讨论了大型语言模型（LLMs）的有害影响，并涵盖了几种可能出现的伤害类型。具体的重点包括：

1. **性能差异**：LLMs在不同群体或不同人口统计特征之间可能会有性能差异。例如，自动语音识别系统可能对黑人说话者的识别性能较差。这些差异可能随着时间的推移被放大，因为反馈循环会导致某些用户产生较少的数据，从而进一步增加差异。
2. **社会偏见和刻板印象**：LLMs可能会延续社会偏见并强化刻板印象。社会偏见涉及将某些概念与特定群体关联起来，而刻板印象则是广泛持有、过度简化且固定的关联。语言在构建、获取和传播刻板印象方面起着重要作用。如果无法理解或解决目标数据中的反刻板印象信息，LLMs在这些数据上的表现可能较差。

在评估LLMs引起的伤害时，考虑社会群体及其历史上的边缘化的重要性。
需要关注受保护属性，这些是不应作为决策基础的人口统计特征（如种族、性别、性取向），同时还要考虑更广泛的社会因素，如历史剥夺和权力不均。
材料通过两个示例来度量和讨论LLMs的有害影响：

1. **名字偏见**：在某些数据集上训练的LLMs可能在理解或回答涉及个人姓名的文本时表现出偏见。例如，模型往往更倾向于预测与他们**熟悉的**知名人物相关的姓名，而对于较不知名的人物，效果较**差**。在测试样本中交换姓名通常不会改变模型的预测结果。
2. **刻板印象**：LLMs倾向于偏好刻板印象的示例而不是反刻板印象的示例。更大的模型通常具有更高的刻板印象得分，表明它们更偏好刻板印象的数据。

衡量和解决这些伤害是一项复杂的任务。现有的公平性指标可能无法同时最小化所有类型的偏见，而对词汇表、解码参数等因素的决策可能对结果产生重大影响。对于LLMs的现有测试基准存在严重批评，并且衡量上游偏见往往不能可靠地预测下游的性能差异和实质性伤害。

1. **行为伤害**：大型语言模型的行为和输出可能对不同群体产生不同程度的伤害。性能差异和社会偏见是两种常见的行为伤害类型。
2. **语言模型与有毒性**：语言模型可能会生成有毒内容，对用户和社交媒体平台都是一个重要的问题。通过提到的工具和API，如Perspective API，可以评估文本的毒性。然而，这些工具也存在一些限制，如无法捕获上下文。
3. **缓解毒性**：文章提到了减轻毒性的策略，包括基于数据和基于解码的方法。然而，这些策略只能部分有效，而且可能会引发其他问题，如对边缘化群体的偏见。
4. **虚假信息和虚假信息战役**：虚假信息问题，虚假信息可以应用在战役中。恶意行为者可能会利用语言模型生成虚假信息，这可能会导致更广泛的虚假信息传播。
5. **内容审查**：语言模型在内容审查方面的可能存在潜在应用，但文章也指出了模型可能被滥用的可能性。内容审查是一个复杂的问题，需要平衡言论自由和有害内容的过滤。
# Task 4 大模型的数据

- **语言模型的行为伤害**（behavioral harm）指的是模型的行为对用户产生的负面影响，而不是模型构造的问题。这包括性能差异和社会偏见与刻板印象两类伤害。
   - **性能差异方面**，研究表明语言识别系统在不同人群之间有准确性差异。例如，针对非洲裔美国英语的性能相对较低。
   - **社会偏见与刻板印象**指的是模型对目标概念与特定人群之间表现出较强的关联性，进而对某些群体产生更明显的影响。例如，自动完成系统对性别做出假设。
- **有毒性（toxicity）和假信息**（disinformation）是语言模型可能产生的另外两种行为伤害。可能的受害者包括使用系统的用户以及接收用户生成内容的人。
   - **有毒性**是指粗鲁、不尊重或不合理的行为，可能使人想要离开对话。定义有毒性需要考虑上下文，不能仅仅依赖词汇列表。
   - **Perspective API**是Google开发的一个有毒性分类的服务，使用机器学习模型为每个输入分配一个毒性分数。然而，它存在一些限制，包括无法捕捉到广泛的语言和社会环境。
   - RealToxicityPrompts是一个评估语言模型生成毒性内容的数据集。通过提供不同的提示，评估模型生成的补全的毒性程度。
   - 在评估语言模型生成内容的毒性时，需要注意实验的设置、评估指标和使用Perspective API的限制。
- **The Pile数据集**，它是由22个高质量数据集组成，包含了825GB的英文文本。研究者发现，使用The Pile数据集训练的GPT-2Pile模型相比使用GPT-3数据集训练的GPT-3模型，可以获得GPT-3数据集未能很好覆盖的信息。
- 数据集的创建需要考虑多个因素，如数据集的组成、数据的收集过程、预处理和清理阶段的工作等
- 人们常常免费放弃个人数据，而大公司从中获得价值和权力。提出了数据联盟的概念，旨在通过中间组织代表数据生产者进行集体谈判，将数据视为劳动而不是财产权。
#  Task 5 大模型的法律问题

1. **新技术与现有法律的关系**： 新兴技术的出现通常引发法律方面的问题。这篇文章提到互联网法等新兴领域，强调了技术和法律之间的相互影响。这表明在开发大型语言模型时，了解当前法律体系的重要性。
2. **互联网的独特挑战**： 互联网的全球性和匿名性使法律执行变得更加复杂。这点提醒我们，大型语言模型在全球范围内部署时需要考虑地理和隐私问题。
3. **法律与道德的区别**： 文章明确区分了法律和道德的不同。这是一个重要的观点，因为大型语言模型的开发者不仅需要遵守法律，还需要考虑道德问题，如隐私和伦理。
4. **法律的管辖权问题**： 指出不同国家和地区的法律可能不同，这对于全球范围内的大型语言模型的部署非常重要。例如，欧盟的GDPR相对于美国的法律更加严格，需要特别的注意。
5. **法律的类型**： 文章提到了普通法、成文法和监管法，这有助于读者了解法律体系的多样性。
6. **数据问题**： 数据是训练大型语言模型的关键，但文章指出了涉及知识产权和隐私方面的问题。这表明开发者需要非常小心处理数据，以避免侵权和隐私问题。
7. **版权法**： 文章详细介绍了版权法，包括许可和公平使用。这对于大型语言模型的开发者非常重要，因为他们需要知道如何合法地使用和引用已有的文本和内容。
8. **公平使用与机器学习**： 讨论了机器学习是否属于公平使用的范畴。这是一个有争议的问题，因为机器学习系统的工作方式可能与传统的版权法不太契合。这是一个需要进一步研究和讨论的问题。
9. **隐私法律**： 文章提到了一些隐私法律，如欧盟的GDPR和加利福尼亚的隐私法案。这些法律对于大型语言模型的数据收集和使用具有重要影响，开发者需要遵守这些法律以确保用户的隐私权。
10. **总结**： 文章总结了开发大型语言模型时需要考虑的法律和伦理问题，并强调了这个领域的快速发展，需要专业知识来做出明智的决策。
# Task 6 大模型的架构
## 6.1 大模型之模型概括
语言模型可以被看做一个黑箱，给定基于自身需求的prompt就可以生成符合需求的结果。
![chrome_jaFJI5A9HE.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694835286800-6c5303ea-76a8-46b6-84cc-6b8551453d79.png#averageHue=%23f0f0f0&clientId=u0148f2d5-d05a-4&from=paste&height=49&id=ua94c54bf&originHeight=55&originWidth=326&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=4872&status=done&style=none&taskId=ua11bce55-5525-423f-8407-abb22665dc2&title=&width=289.77777777777777)
从数学的⻆度考虑就对训练数据 (traing data: （x1,...,xL ）)的概率分布：
![chrome_0A1GwKcJU9.png](https://cdn.nlark.com/yuque/0/2023/png/29540661/1694835342561-d0ec49c2-1e21-43b6-8ae5-59414a979169.png#averageHue=%23eeeeee&clientId=u0148f2d5-d05a-4&from=paste&height=40&id=uf66100b1&originHeight=45&originWidth=417&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=5787&status=done&style=none&taskId=ube0d394b-28b7-4f9e-8fc0-02d8631388d&title=&width=370.6666666666667)
 在学习内容中中，我们将彻底揭开⾯纱，讨论⼤型语⾔模型是如何 构建的。今天的内容将着重讨论两个主题，分别是分词和模型架 构： 

- **分词**：即如何将⼀个字符串拆分成多个标记。 
- **模型架构**：我们将主要讨论Transformer架构，这是真正实现⼤型语⾔模型的建模创新。  
## 6.2 分词
**基于空格的分词**
英语可以直接分词
**Byte pair encoding**
在字节对编码（Byte Pair Encoding, BPE）算法中，它通常应用于数据压缩领域，并被广泛用作一种分词器。BPE分词器通过训练模型数据来学习，并获得需要进行分词的文本的一些频率特征。
学习BPE分词器的过程大致如下：

1. 输入：训练语料库（字符序列）。
2. 初始化词汇表（Vocabulary）为字符的集合。
3. 当我们希望词汇表继续增长时：
   - 寻找在语料中共同出现次数最多的元素对。
   - 使用一个新的符号来替换所有这些共同出现的元素对。
   - 将新生成的符号添加到词汇表中。

**Unigram模型**
Unigram模型是一种基于原则的中文分词方法。它使用训练数据中词汇的出现次数来估计词汇的概率，并根据概率计算分词结果的似然值。通过迭代优化和剪枝，Unigram模型可以提高分词的准确性和性能。
## 6.3 模型架构
1. 语言模型定义与分类：

- 语言模型被定义为对标记序列的概率分布。
- 分为三个类型：编码端（Encoder-Only）、解码端（Decoder-Only）和编码-解码端（Encoder-Decoder）。

2. 编码端（Encoder-Only）架构：

- 代表模型如BERT和RoBERTa。
- 生成上下文向量表示，不能直接用于生成文本。
- 主要用于分类任务，如情感分析和自然语言理解任务。
- 优势在于对文本的上下文信息有更好的理解，多用于理解任务。
- 上下文向量表示可以双向依赖于左侧和右侧上下文。

3. 解码器（Decoder-Only）架构：

- 代表模型如GPT系列。
- 适用于生成完整文本，如自动补全任务。
- 上下文向量表示只能单向依赖于左侧上下文。
- 简单训练目标，如最大似然。

4. 编码-解码端（Encoder-Decoder）架构：

- 代表模型如Transformer、BART、T5等。
- 允许双向上下文向量表示处理输入，同时生成输出。
- 适用于表格到文本生成等任务。
- 上下文向量表示可以双向依赖于左侧和右侧上下文。

5. 语言模型理论：

- 介绍了基本架构的构建，包括词嵌入、递归神经网络（RNN）、Transformer等。
- 强调了残差连接、层归一化和位置嵌入的重要性。

6. 注意力机制（Attention Mechanism）：

- 重要的是理解注意力机制，用于计算标记之间的关联性。
- 可以包括多头注意力来处理不同方面的关联。

7. GPT-3（Generative Pre-trained Transformer 3）：

- GPT-3架构由多个Transformer块堆叠而成。
- 具有大量参数，包括隐藏状态维度、前馈层维度、注意头数量和上下文长度。
- 使用了Sparse Transformer来减少参数数量，与稠密层交替使用。
- 架构类型与训练目标会影响模型的性能和用途。
# Task 7 模型训练
本章内容主要涵盖了训练大语言模型的目标函数和优化算法，以及针对不同类型的语言模型（Decoder-only、Encoder-only、Encoder-decoder）的训练方法。下面是章节的主要内容和要点：
7.1 目标函数 在这一部分，讨论了三种不同类型的语言模型的目标函数：

1. Decoder-only 模型（如 GPT-3）：生成单向上下文嵌入，一次生成一个标记。
2. Encoder-only 模型（如 BERT）：生成双向上下文嵌入。
3. Encoder-decoder 模型（如 T5）：编码输入，解码输出。

不论使用哪种模型，都可以使用不同的架构（如 LSTM、Transformer）将标记序列映射到上下文嵌入中。

- 7.1.1 Decoder-only 模型：
   - 自回归语言模型定义了一个条件分布，其中每个标记的生成都依赖于前面生成的标记。
   - 目标是最大化条件概率，以最大似然估计为基础来训练模型。
- 7.1.2 Encoder-only 模型：
   - BERT 是一个代表 Encoder-only 模型的例子，它有两个目标函数：掩码语言建模和下一句预测。
   - 掩码语言建模是通过预测缺失的标记来进行训练，类似于自动编码器。
   - 下一句预测是通过判断两个句子是否连续来训练模型，以学习句子间的关系。
   - 训练数据集是通过构建句子对的方式得到的。
- 7.1.3 Encoder-decoder 模型：
   - 这种模型适用于生成型任务，例如表格到文本的转换。
   - 通过首先对输入进行双向编码，然后进行自回归解码来完成任务。

7.1.2.2.1 RoBERTa：

- RoBERTa 是对 BERT 模型的改进，删除了下一句预测任务。
- 训练 RoBERTa 使用更多的数据和更长的时间，提高了性能。

7.1.3.1 BART (Bidirectional Auto-Regressive Transformers)：

- BART 是基于 Transformer 架构的编码器-解码器模型。
- 通过掩码标记来预测序列中的标记，类似于 BERT。
- 在分类和生成任务上都表现出了强大的性能。

7.1.3.2 T5 (Text-to-Text Transfer Transformer)：

- T5 是另一种基于 Transformer 架构的编码器-解码器模型。
- 在预训练任务中，它使用了一种“Text-to-Text”框架，将不同的 NLP 任务都转化为文本生成任务。
- T5 通过多目标学习统一了各种任务，包括分类和生成。

4.2 优化算法
在这一部分，讨论了训练大语言模型时使用的不同优化算法和技术。
4.2.1 随机梯度下降 (SGD)：

- 随机梯度下降是最简单的优化算法之一，通过随机采样小批量数据来更新模型参数。
- 优化过程需要考虑参数快速收敛、数值稳定性和内存效率等因素。

4.2.2 Adam (adaptive moment estimation)：

- Adam 算法引入了动量和自适应学习率的概念，可以更有效地收敛。
- 每个参数维度都有自适应的步长。
- 存储占用略高。

4.2.3 AdaFactor：

- AdaFactor 是一种旨在减少存储占用的优化算法。
- 不存储二阶矩阵，而是存储行和列的和来重构矩阵。
- 用于训练 T5 模型。

4.2.4 混合精度训练：

- 混合精度训练是一种减少存储占用的方法。
- 将主权重存储在 FP32 中，执行其他操作时使用 FP16。
- 损失缩放用于调整损失，以避免梯度过小。

4.2.5 学习率：

- 学习率通常随时间衰减，但对于 Transformer 模型，需要使用预热（warmup）来提高学习率。
- 预热可以防止梯度消失问题。

4.2.6 初始化：

- 初始化权重对于模型的训练至关重要。
- 不同的模型使用不同的初始化方法，如 Xavier 初始化、缩放权重等。
# Task 8 分布式训练
**9.1 为什么分布式训练越来越流行**

- 深度学习在多个领域得到广泛应用，但模型规模不断增大，例如GPT-3模型参数数量高达1750亿。
- 即使使用强大的硬件如1024张80 GB的A100 GPU，完整训练一个GPT-3模型也需要一个月。
- 模型规模的增加对计算资源，特别是算力和内存，提出了更高的要求。
- 由于内存墙和物理定律的限制，单一设备的性能和内存容量无法满足大型模型的需求。
- 为了弥补算力不足的问题，分布式训练成为一种解决方案，通过使用多节点集群来提高计算性能，分布式训练变得日益重要。

**9.2 常见的并行策略**

- 简单的机器堆叠不能有效提升计算性能，因为深度学习训练涉及到计算和数据传输之间的协调，不仅仅是将任务分配给多个设备。
- 数据并行和模型并行是两种主要的并行策略，它们在数据和模型的分割方式上有所不同。

**9.2.1 数据并行**

- 数据并行将数据切分成多份，每个设备上的模型是相同的。
- 梯度需要在反向传播过程中进行AllReduce操作，以确保各设备上的模型保持一致。
- 适用于数据集较大、模型较小的情况，例如视觉分类模型（如ResNet50）。

**9.2.2 模型并行**

- 模型并行将模型切分成多份，每个设备上的数据是完整的、一致的。
- 由于每个设备需要完整的数据输入，因此需要进行数据广播。
- 适用于非常庞大的模型，可能无法容纳在单个计算设备上的情况，例如语言模型（如BERT）。

**9.2.3 流水并行**

- 流水并行将神经网络划分为多个阶段，分发到不同的计算设备上，各设备之间以接力方式完成训练。
- 适用于超大型模型，无法在单个设备上存放的情况。

**9.2.4 混合并行**

- 在网络训练中，也可以混合多种并行策略，以充分利用计算资源。
- 例如，GPT-3采用了流水并行、数据并行和模型并行的混合策略，充分发挥不同策略的优势。
- 并行策略的选择会影响训练效率，框架对并行训练的支持程度也会影响算法工程师的开发效率。

**总结**

- 分布式训练在面对越来越庞大的深度学习模型时变得愈发流行，但选择合适的并行策略对于提高训练效率至关重要。
- 不同的问题和模型可能需要不同的并行策略，需要根据具体情况进行选择和调整。
- 深度学习框架的支持程度对于实现并行训练也具有重要影响，因此需要综合考虑多个因素来优化训练流程。
# Task 9 新的模型架构
介绍了模型架构中的混合专家和基于检索的模型，这些方法旨在解决扩大语言模型规模的问题。本章讨论了这些方法的原理、实验和优势，下面是我的学习笔记和一些个人见解：
### 1. 模型架构概述

- 在神经语言模型中，核心接口是一个将token序列映射到上下文嵌入的编码器。
- 以GPT-3为例，它是一个由96层Transformer block堆叠而成的神经语言模型，每一层都使用了自注意力层和前馈层。
### 2. 需要扩大模型规模的挑战

- 扩大模型规模需要更多的数据、更多的模型参数和更多的计算资源。
- 目前的模型规模已经达到了极限，需要重新思考如何构建更大的语言模型。
### 3. 混合专家模型
#### 3.1 基础知识

- 混合专家的思想是将多个专家应用于不同的输入，每个专家具有自己的参数和嵌入。
- 通过专家的混合，生成最终的模型输出。
#### 3.2 示例

- 可以考虑一个问题预测任务，使用前馈神经网络。
- 传统方法是增加网络的宽度或深度，但混合专家方法是定义多个专家，每个专家具有不同的嵌入和参数。
- 最终的输出是这些专家的混合。
#### 3.3 训练和节约计算

- 混合专家模型的训练可以通过反向传播进行，需要更新专家的参数和混合权重。
- 为了节约计算，可以将混合权重函数近似为仅使用前几个专家，而不是所有的专家。
#### 3.4 平衡专家

- 为了确保混合专家模型的有效性，需要确保所有专家都能被输入使用。
- 如果只有少数几个专家被激活，会浪费其他未使用的专家的计算资源。
#### 3.5 并行性

- 混合专家模型适合并行化，每个专家可以放置在不同的机器上。
- 中心节点计算混合权重，只有包含激活专家的机器会处理计算。
#### 3.6 混合专家模型的变体

- 不同研究团队提出了不同的混合专家模型变体，如Sparsely-gated Mixture of Experts、Switch Transformer、GLaM、FacebookMoE等。
### 4. 基于检索的模型

- 基于检索的模型适合知识密集型任务，其中存储库中的文本可以用于提供上下文信息。
#### 4.1 编码器-解码器

- 序列到序列任务可以使用编码器-解码器模型，例如BART和T5。
#### 4.2 检索方法

- 基于检索的模型首先检索与输入相关的序列，然后生成输出。
- 最近邻方法是其中一种常用的检索方法。
#### 4.3 Retrieval-augmented generation (RAG)

- RAG模型将检索到的序列与输入传递给生成器，以生成输出。
- 通过检索可以提高模型性能，特别是在问题生成任务上。
#### 4.4 其他基于检索的模型

- 还有其他基于检索的模型如RETRO，它使用冻结的BERT进行检索。
### 5. 总结和个人见解

- 混合专家和基于检索的模型是扩大语言模型规模的有效方法。
- 但在不同的任务和规模上，这些模型的性能很难进行直接比较。
- 混合专家模型对权力下放有重要影响，可以分布在多台机器上进行计算。
- 基于检索的模型适合知识密集型任务，提供了可解释性和动态更新存储库的能力。

这些模型的发展使得构建更大规模的语言模型成为可能，但在实际应用中，仍然需要谨慎考虑任务需求、资源限制和模型性能的平衡。未来的研究可能会进一步推动这些模型的发展，以适应更多领域的需求。
# Task 10 大模型之Adaptation
通过微调可以在特定任务上优化预训练的语言模型。微调使用预训练模型的参数作为初始化，并将任务特定的参数添加到模型中。在微调过程中，通常使用比预训练时小一个数量级的学习率，并且微调的时间远远少于预训练过程。微调的目标是在特定任务上优化模型的性能。
通过微调，我们可以利用预训练模型学习到的通用语言表示能力，并将其适应到特定的下游任务中。微调主要涉及以下几个步骤：

1. 加载预训练模型：首先，我们加载预训练的语言模型，例如BERT或GPT-3等，并使用这些模型的参数作为初始化。
2. 创建下游任务数据集：然后，我们准备用于微调的下游任务数据集。该数据集通常包含输入文本和相应的标签或目标输出。
3. 添加任务特定的头部：为了适应特定的下游任务，我们为模型添加任务特定的头部，例如分类器、序列标注器或生成器等。这些头部与模型的预训练部分连接在一起，在训练过程中一起进行优化。
4. 定义任务损失函数：我们需要定义适用于特定任务的损失函数，用于衡量模型在该任务上的性能。常见的损失函数包括交叉熵损失、均方误差等。
5. 进行微调训练：使用下游任务数据集和任务损失函数，我们对模型进行微调训练。在训练过程中，我们调整模型参数，使其能够更好地适应特定任务。

通过微调，模型可以通过在特定任务上进行优化来提高性能。与过去相比，微调使得模型可以更好地理解特定领域的语言和任务要求。然而，微调的成功还取决于数据集的质量、模型的架构选择以及适当的超参数设置等因素。
# Task 11 大模型对环境的影响
本章讨论了大语言模型对环境的影响，重点关注了气候变化和温室气体排放。气候变化已经对我们的环境产生严重影响，例如全球平均表面温度升高、自然灾害增多和海平面上升等。而训练大语言模型所需的能源使用也导致二氧化碳排放增加。一些数据表明，训练一个大型语言模型可以排放数百吨二氧化碳。本章的学习目标包括全面了解大语言模型对环境的影响，能够计算特定语言模型的排放量，并提高对环境影响的认识以及个人的责任意识。
在生命周期评估中，通过考虑语言模型的整个生命周期，包括生产、使用和终结阶段，可以更全面地评估其环境影响。生产阶段涉及原材料提取、制造和运输过程，使用阶段涉及设备的能耗，而寿命终结阶段则涉及设备的拆除、回收或处置。需要从系统的角度思考，避免解决一个问题引发多个新问题的情况。此外，大型数据中心的建设和维护也会产生环境影响，如水资源消耗和化学物质排放。
在能源使用和温室气体排放方面，碳强度是关键因素。煤炭和天然气等化石燃料产生的碳排放最多，但考虑整个生命周期时，其他绿色能源也会产生排放。通过电力的碳强度和供应组合（如煤炭和水电）可以估算数据中心的排放量。对于减轻环境影响，使用清洁能源以及提高能源效率非常重要。
为了估算训练模型的排放量，可以使用ML CO2 Impact Calculator和其他研究的数据。不同模型的能源使用量和排放量会有所不同，例如BERT-base和神经结构搜索模型。准确测量训练模型的排放量对于了解和减轻环境影响至关重要。
需要注意的是，提供的所有数据都是估计值，因为缺乏对数据中心的监控和信息。此外，环境成本是一个复杂的问题，因为很难进行信用和责任分配。然而，我们应该意识到大语言模型的发展不仅带来了巨大的效益，也需要考虑环境成本。因此，我们需要采取行动来监测和减轻环境影响，并促使技术创新和可持续发展相结合。
个人见解：
本章提醒了我们在技术发展方面需要对环境影响负责。虽然大语言模型在语言处理任务中取得了显著的进展，但我们不能忽视其对能源消耗和温室气体排放的负面影响。在追求技术进步和创新的同时，我们应该思考如何降低训练模型的环境成本。这可以包括使用更清洁的能源、提高能源效率、改善数据中心的设计和管理等方面。
此外，环境成本的分担也是一个重要的问题。尽管训练大语言模型的成本通常由大型技术公司承担，但我们也应该认识到这种成本分担不够均衡，可能进一步加剧贫困和社会不平等问题。因此，在减轻环境影响的同时，我们也应该关注公平性和社会责任，确保环境成本不会全面落在弱势群体身上。
